{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/cw2c7r3o20w9zn8gkecaeyjhgw3xdgbj.png\" width = 400, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 5> Logistic Regression with Python</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this notebook, you will learn Logist Regression, and then, you'll create a model with telecommunications data to predict when its customers will leave for a competitor, so that you can take some action to retain the customer.\n",
    "\n",
    "\n",
    "<a id=\"ref1\"></a>\n",
    "## What is different between Linear and Logistic Regression?\n",
    "\n",
    "While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate a classification, we need some sort of guidance on what would be the **most probable class** for that data point. For this, we use **Logistic Regression**.\n",
    "\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>Recall linear regression:</strong></font>\n",
    "<br>\n",
    "<br>\n",
    "    Linear regression finds a function that relates a continuous dependent variable, <b>y</b>, to some predictors (independent variables $x_1$, $x_2$, etc.). Simple linear regression assumes a function of the form:\n",
    "<br><br>\n",
    "$$\n",
    "y = w_0 + w_1  x_1 + w_2  x_2 + \\cdots \n",
    "$$\n",
    "<br>\n",
    "and finds the values of $w_0$, $w_1$, $w_2$, etc. The term $w_0$ is the \"intercept\" or \"constant term\" :\n",
    "<br><br>\n",
    "$$\n",
    "Y = W^TX\n",
    "$$\n",
    "<p></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, _y_, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
    "\n",
    "Despite the name logistic _regression_, it is actually a __probabilistic classification__ model. Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function:\n",
    "\n",
    "$$\n",
    "‚Ñé_w(ùë•) = \\frac {e^{(w_0 + w_1  x_1 + w_2  x_2 +\\cdots)}}{1 + e^{(w_0 + w_1  x_1 + w_2  x_2 + \\cdots)}}\n",
    "$$\n",
    "Or:\n",
    "$$\n",
    "p(X) = ProbabilityOfaClass =  P(Y=1|X) = \\sigma({W^TX}) = \\frac{e^{W^TX}}{1+e^{W^TX}} = exp({W^TX}) / (1+exp({W^TX}))  \n",
    "$$\n",
    "Or:\n",
    "$$\n",
    "Logit (ProbabilityOfaClass) = log(oddRatio) = log(\\frac{p(X)}{1-p(X)}) = w_0 + w_1  x_1 + w_2  x_2 + \\cdots\n",
    "$$\n",
    "which produces p-values between 0 (as y approaches minus infinity) and 1 (as y approaches plus infinity). This now becomes a special kind of non-linear regression.\n",
    "\n",
    "In this equation, ${W^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(W^TX)$ is the [logistic function](http://en.wikipedia.org/wiki/Logistic_function), also called logistic curve. It is a common \"S\" shape (sigmoid curve), and was first developed for modelling population growth.\n",
    "\n",
    "\n",
    "So, briefly, Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability:\n",
    "\n",
    "<img\n",
    "src=\"https://ibm.box.com/shared/static/kgv9alcghmjcv97op4d6onkyxevk23b1.png\" width = \"400\" align = \"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets first import requiered libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer churn with Logistic Regression\n",
    "A telecommunications company is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you‚Äôre an analyst at this company and you have to find out who is leaving and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### About dataset\n",
    "We‚Äôll use a telecommunications data for predicting customer churn. This is a historical customer data where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it‚Äôs less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company. \n",
    "\n",
    "\n",
    "This data set provides info to help you predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n",
    "\n",
    "\n",
    "\n",
    "The data set includes information about:\n",
    "\n",
    "- Customers who left within the last month ‚Äì the column is called Churn\n",
    "- Services that each customer has signed up for ‚Äì phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "- Customer account information ‚Äì how long they‚Äôve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "- Demographic info about customers ‚Äì gender, age range, and if they have partners and dependents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "###  Load the Telco Churn data \n",
    "Telco Churn is a hypothetical data file that concerns a telecommunications company's efforts to reduce turnover in its customer base. Each case corresponds to a separate customer and it records various demographic and service usage information. Before you can work with the data, you must use the URL to get the ChurnData.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Click here and press Shift+Enter\n",
    "!wget -O ChurnData.csv https://ibm.box.com/shared/static/8s8dn9gam7ipqb42cm4aehmbb26zkekl.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Load Data From CSV File  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets select some features for the modeling. Also we change the target data type to be integer, as it is a requirement  by the skitlearn algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": true,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### How many rows, columns in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "churn_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define X, and y for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(churn_df['churn'])\n",
    "y [0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we normalize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we split our dataset into train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (Logistic Regression with Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#y_cat = churn_df['churn']\n",
    "# X_train, X_test, y_train, y_test = train_test_split( X, y_cat, test_size=0.2, random_state=4)\n",
    "# print ('Train set:', X_train.shape,  y_train.shape)\n",
    "# print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=0.01).fit(X_train,y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict using our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = LR.predict(X_test)\n",
    "yhat [0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print (classification_report(y_test, yhat))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try jaccard index for accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_similarity_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Numpy\n",
    "\n",
    "- Fitting using Gradient descent implemented by Gradient Descent\n",
    "- Fitting using BFGS algorithm implemented by Scipy library\n",
    "- Fitting using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Fitting using Gradient descent implemented by Gradient Descent\n",
    "We should find the best parameters for our model by minimizing the cost function of our model. In other word to minimize J value that we just defined in __cost()__ function.\n",
    "\n",
    "How to minimize the cost function? \n",
    "\n",
    "The answer is, using an optimization approach. There are different optimization approaches, but we use one of famous and effective approaches here, gradient descent.\n",
    "\n",
    "What is gradient descent?\n",
    "Generally, gradient descent  is an iterative approach to finding the minimum of a function. Specifically, in our case, gradient descent is a technique to use derivative of a cost function to change the parameter values, to minimize the cost/error. \n",
    "\n",
    "Lets see how it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, Sigmoid function‚Äôs output is always between 0 and 1, which make it proper to interpret the results as probabilities.\n",
    "It is obvious that, when the outcome of sigma function get closer to 1, the probability of y=1, given x, goes up, and in contrast, when the sigmoid value is closer to zero, the probability of y=1, given x, is very small.\n",
    "Here we define a function to compute the sigmoid of an input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the sigmoid function\n",
    "def sigmoid(z):\n",
    "    s =  1.0 / (1.0 + np.exp(- z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to compare the output of our model with the actual label of the customer. \n",
    "Then, record the difference as our model‚Äôs error for each customer. The total error (for all customers) is cost of your model, and is calculated by model‚Äôs cost function. The cost function, by the way, basically represents how to calculate the error of the model, which is the difference between actual and the model‚Äôs predicted values. \n",
    "However, Logistic regression, uses a specific cost function which penalizes situations in which the class is 0 and the model output is 1, and vice versa. It uses log-likelihood to form the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computes cost given predicted and actual values\n",
    "def cost_computation(theta, X, y):\n",
    "    hx = sigmoid(np.dot(X, theta)) # predicted probability of label 1\n",
    "    cost = (-y)* np.log(hx) - (1-y)*np.log(1-hx) # log-likelihood vector\n",
    "    J = cost.mean()\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the curve by calculating the gradients or the first derivatives of the cost function with respect to each theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_computation(theta, X, y):\n",
    "    hx = sigmoid(np.dot(X, theta))\n",
    "    error = hx - y # difference between label and prediction\n",
    "    grad = np.dot(error, X) / y.size # gradient vector\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function predicts whether the label is 0 or 1 using learned logistic regression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_class(theta, X):\n",
    "    m, n = X.shape\n",
    "    p = np.zeros(shape=(m, 1))\n",
    "\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "\n",
    "    for it in range(0, h.shape[0]):\n",
    "        if h[it] > 0.5:\n",
    "            p[it, 0] = 1\n",
    "        else:\n",
    "            p[it, 0] = 0\n",
    "\n",
    "    return p.reshape(m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, iterations):\n",
    "#gradient descent algorithm to find optimal theta values\n",
    "    theta_n = theta.size  \n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(np.dot(x, theta))\n",
    "        gradient_val = grad_computation(theta, x, y)\n",
    "        theta= theta - alpha * gradient_val\n",
    "        print('>iteration=%d, lrate=%.3f, cost=%.3f' % (i, alpha, cost_computation(theta, x, y)))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we add 1 to as first value of each parameter vector, to play intrecept of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = np.append( np.ones((X_train.shape[0], 1)), X_train, axis=1)\n",
    "X_test_1 = np.append( np.ones((X_test.shape[0], 1)), X_test, axis=1)\n",
    "X_train_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix an extra column of ones to the feature matrix (for intercept term)\n",
    "theta_0 = 0.1* np.random.randn(X_train_1.shape[1])\n",
    "theta = gradient_descent(X_train_1, y_train, theta_0, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fitting using bfgs algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can use __fmin_bfgs__ to minimize the cost function. __fmin_bfgs__  is a scipy built-in function which finds the best parameters theta for the logistic regression cost function given a fixed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = opt.fmin_bfgs(cost_computation, theta_0, fprime=grad_computation, args=(X_train_1, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Fitting using Stochastic Gradient Descent\n",
    "\n",
    "What if we estimate gradient with just one sample?  \n",
    "\n",
    "Gradient Descent is the process of minimizing our cost function by following the gradients of the cost function. \n",
    "\n",
    "'Stochastic Gradient Descent' is an optimization algorithem where we update the coefficients of the model in every iteration to minimize the error of a model on the training data. The way this algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "In this function we calculate the error for each prediction and update the theta accordingly. The error is calculated as the difference between the predication value and the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate h_theta -- Predictionof a row\n",
    "def predict_row(row, theta):\n",
    "    hx = sigmoid(np.dot(row, theta))\n",
    "    return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate theta using stochastic gradient descent\n",
    "def theta_sgd(X_train, y_train, alpha, n_epoch):\n",
    "    theta = [0.0 for i in range(len(X_train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for x,y in zip(X_train_1,y_train):\n",
    "            ht = predict_row(x, theta)\n",
    "            error =  ht - y\n",
    "            theta[0] = theta[0] - alpha * error \n",
    "            for i in range(len(theta)-1):\n",
    "                theta[i + 1] = theta[i + 1] - alpha * error  * x[i+1]\n",
    "        sum_error += error**2\n",
    "        # cost computation\n",
    "        cost = cost_computation(theta, X_train, y_train)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f, cost=%.3f' % (epoch, alpha, sum_error,cost))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = theta_sgd(X_train_1,y_train, 0.001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_class(np.array(theta), X_train_1)\n",
    "#Compute accuracy on our training set\n",
    "print ('Train Accuracy: %f' % ((y_train[(pred == y_train)].size / float(y_train.size)) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_class(np.array(theta), X_test_1)\n",
    "#Compute accuracy on our training set\n",
    "print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a parameter, regularization, that is used for preventing over fitting. We can find the best regularization value using accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization_Inv=[0.00001,0.1,1,100]\n",
    "Length=len(Regularization_Inv)\n",
    "mean_acc=np.zeros((Length))\n",
    "std_acc=np.zeros((Length))\n",
    "ConfustionMx=[];\n",
    "\n",
    "\n",
    "for Reg,n in zip(Regularization_Inv,range(0,Length)):\n",
    "    \n",
    "    LR = LogisticRegression(C=Reg).fit(X_train,y_train)\n",
    "    yhat=LR.predict(X_test)\n",
    "    mean_acc[n]=np.mean(yhat==y_test);\n",
    "    \n",
    "    \n",
    "    std_acc[n]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "    ConfustionMx.append(confusion_matrix(yhat,y_test,labels=[1,0]))\n",
    "mean_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The best accuracy for Logistic regression is\", mean_acc.max()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "IBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems ‚Äì by your enterprise as a whole. A free trial is available through this course, available here: [SPSS Modeler](http://cocl.us/ML0101EN-SPSSModeler).\n",
    "\n",
    "Also, you can use Watson Studio to run these notebooks faster with bigger datasets. Watson Studio is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, Watson Studio enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of Watson Studio users today with a free account at [Watson Studio](https://cocl.us/ML0101EN_DSX)\n",
    "\n",
    "### Thanks for completing this lesson!\n",
    "\n",
    "Notebook created by: <a href = \"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Copyright &copy; 2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).‚Äã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
